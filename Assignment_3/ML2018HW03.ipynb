{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment -- 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n",
    "\n",
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><span style=\"color:red;\">**IMPORTANT NOTICE FOR THIS PART**</span></center></h1>\n",
    "\n",
    "Before submitting, **please**, make sure that your notebook runs **without errors** in Python 3.6\n",
    "and **reproduces your solution as intended**, when you **Restart the Kernel and re-run the whole\n",
    "notebook**!\n",
    "<span style=\"color:red;\">You will be severely penalized if you notebook does not run.</span>\n",
    "\n",
    "Where it is possible, your solution will be graded based on **plots**, generated by **your code** on **TA's** laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 (62 pt.): Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you are asked to complete a couple of practical tasks and prove an important theoretical result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 pt.):  A warmup with simple PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will remember how to use Principal Component Analysis for artficial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (2 pt.)\n",
    "\n",
    "* Load data using `sklearn.datasets.make_blobs` generator ([reference](http://scikit-learn.org/stable/modules/classes.html#samples)). Choose 1000 observations, and 100 dimensions.  \n",
    "* Visualize data for the first two dimensions and color them according to their class. \n",
    "* Apply PCA so that 90% of the variance is explained. Output the number of principal components. \n",
    "* Visualize the transformed data for the first 2 components. Is it linearly separable?  <b>(YES)</b>\n",
    "* Apply KernelPCA with RBF kernel with 2 components. Choose `gamma` in order to obtain linear separability. Plot data on first 2 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(n_samples=1000, n_features=100, centers=3, random_state=42)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs)\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(0.9)\n",
    "X_blobs_pca = pca.fit_transform(X_blobs)\n",
    "print (\"PCA components = {}\".format(pca.n_components_))\n",
    "\n",
    "plt.scatter(X_blobs_pca[:, 0], X_blobs_pca[:, 1], c=y_blobs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(0.01,1,10):\n",
    "    kpca = KernelPCA(kernel=\"rbf\", gamma=i, n_components= 2, random_state= 42)\n",
    "    X_blobs_kpca = kpca.fit_transform(X_blobs) \n",
    "    plt.scatter(X_blobs_kpca[:, 0], X_blobs_kpca[:, 1], c=y_blobs)\n",
    "    plt.title('Gamma = {}'.format(i))\n",
    "    plt.show()\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"rbf\", gamma=0.01, n_components= 2, random_state= 42 )\n",
    "X_blobs_kpca = kpca.fit_transform(X_blobs)\n",
    "plt.scatter(X_blobs_kpca[:, 0],X_blobs_kpca[:,1], c=y_blobs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gamma = 0.01 </b>(Linear Separable case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 (3 pt.)\n",
    "\n",
    "* Load data using `sklearn.datasets.make_circles` generator ([reference](http://scikit-learn.org/stable/modules/classes.html#samples)). Choose 1000 observations, noise=0.1, factor=0.3.\n",
    "* Visualize the data and color points them according to their class.\n",
    "* Apply the PCA with 1 principal component. Print and comment on the explained variance ratio.\n",
    "* Visualize the transformed data for the first principal component. Is it linearly separable? \n",
    "* Apply the KPCA with the RBF kernel with 1 component. Select the best `gamma` parameter for the kernel.\n",
    "    * For this, plot the projection of the data on the first principal component. What is the first `gamma` when data become linearly separable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles)\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(1)\n",
    "X_circles_pca = pca.fit_transform(X_circles)\n",
    "print (\"PCA(1) explained variance = {}\".format(pca.explained_variance_ratio_[0]))\n",
    "\n",
    "plt.scatter(X_circles_pca[:, 0], np.zeros_like(X_circles_pca), c=y_circles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not linearly separable, because it is just projection on axis x and we have circles data(variance_ration ~ 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(1,2,35):\n",
    "    kpca = KernelPCA(kernel=\"rbf\", gamma=i, n_components= 1, random_state= 42)\n",
    "    X_circles_kpca = kpca.fit_transform(X_circles) \n",
    "    plt.scatter(X_circles_kpca[:, 0], np.zeros_like(X_circles_kpca), c=y_circles)\n",
    "    plt.title('Gamma = {}'.format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"rbf\", gamma=1.63, n_components= 1, random_state= 42)\n",
    "X_circles_kpca = kpca.fit_transform(X_circles) \n",
    "plt.figure(figsize= (20,1))\n",
    "plt.scatter(X_circles_kpca[:, 0], np.zeros_like(X_circles_kpca), c=y_circles)\n",
    "plt.title('Gamma = 1.63')\n",
    "plt.show()\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (10 pt.):  Manifold learning\n",
    "\n",
    "Try Kernel PCA, Laplacian Eigenmaps, Locally Linear Embedding and Isomap to find the best representation of 3D Datasets composed of 1000 points each.\n",
    "\n",
    "In this task, you must try to determine which is the most suitable non-linear projection method to uncover the ’true’ low-dimensional representation of 3D dataset. To address this you need to answer the following questions:\n",
    "* What is the underlying manifold that we want to uncover?\n",
    "* Once we know what low-dimensional representation to expect, which method seems the most suitable?\n",
    "* For each method, what are the necessary hyper-parameters we have to tune and how can we select them?\n",
    "\n",
    "So, try out Kernel PCA, Laplacian Eigenmaps, Locally Linear Embedding and Isomap with different parameters, plot the embedded data and comment on which method works better and why, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, color = make_s_curve(1000, noise = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1)\n",
    "ax = Axes3D(fig, elev=-160, azim=110)\n",
    "# pca = PCiginal = pca.inverse_transform(X_iris_transformed)\n",
    "ax.scatter(X[:,0], X[:,1], X[:, 2], c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel PCA\n",
    "kernels = ['linear','poly','rbf','cosine']\n",
    "fig, axes = plt.subplots(ncols=4, nrows=1, figsize=(20, 5))\n",
    "\n",
    "for ker, ax in zip(kernels, axes.flat):\n",
    "    kpca = KernelPCA(kernel=ker, gamma=2, n_components= 2, random_state= 42,fit_inverse_transform=True)\n",
    "    X_kpca = kpca.fit_transform(X)\n",
    "    ax.scatter(X_kpca[:, 0], X_kpca[:,1], c=color)\n",
    "    ax.set_title('Kernel PCA ({}) representation'.format(ker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectral Embedding (Laplacian eigenmaps)\n",
    "neighbours = [5,20,50,100]\n",
    "fig, axes = plt.subplots(ncols=len(neighbours), nrows=1, figsize=(20, 5))\n",
    "\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    se = manifold.SpectralEmbedding(n_components=2, n_neighbors=n)\n",
    "    X_se = se.fit_transform(X)\n",
    "    ax.scatter(X_se[:, 0], X_se[:,1], c=color)\n",
    "    ax.set_title('neighbours = {}'.format(n))\n",
    "plt.suptitle('Laplacian eigenmaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLE (standard)\n",
    "neighbours = [10,20,30,50,70,100]\n",
    "fig, axes = plt.subplots(ncols=len(neighbours)//2, nrows=2, figsize=(25,12))\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    lle = manifold.LocallyLinearEmbedding(n_neighbors = n,n_components=2,method='standard',random_state=42)\n",
    "    X_lle = lle.fit_transform(X)\n",
    "    ax.scatter(X_lle[:, 0], X_lle[:,1], c=color)\n",
    "    ax.set_title('{}, er: {}'.format(n, lle.reconstruction_error_.round(9)))\n",
    "plt.suptitle('LLE (standard)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLE (ltsa)\n",
    "neighbours = [15,20,30,50,70,100]\n",
    "fig, axes = plt.subplots(ncols=len(neighbours)//2, nrows=2, figsize=(25,12))\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    lle = manifold.LocallyLinearEmbedding(n_neighbors = n,n_components=2,method='ltsa',random_state=42)\n",
    "    X_lle = lle.fit_transform(X)\n",
    "    ax.scatter(X_lle[:, 0], X_lle[:,1], c=color)\n",
    "    ax.set_title('{}, er: {}'.format(n, lle.reconstruction_error_.round(7)))\n",
    "plt.suptitle('LLE (ltsa)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LLE (hessian)\n",
    "neighbours = [30,65,100,130]\n",
    "fig, axes = plt.subplots(ncols=len(neighbours)//2, nrows=2, figsize=(25,12))\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    lle = manifold.LocallyLinearEmbedding(n_neighbors = n,n_components=2,method='hessian',random_state=42)\n",
    "    X_lle = lle.fit_transform(X)\n",
    "    ax.scatter(X_lle[:, 0], X_lle[:,1], c=color)\n",
    "    ax.set_title('{}, er: {}'.format(n, lle.reconstruction_error_.round(5)))\n",
    "plt.suptitle('LLE (hessian eigenmap)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLE (modified)\n",
    "neighbours = [25,50,70,100]\n",
    "fig, axes = plt.subplots(ncols=len(neighbours)//2, nrows=2, figsize=(25,12))\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    lle = manifold.LocallyLinearEmbedding(n_neighbors = n,n_components=2,method='modified',random_state=42)\n",
    "    X_lle = lle.fit_transform(X)\n",
    "    ax.scatter(X_lle[:, 0], X_lle[:,1], c=color)\n",
    "    ax.set_title('{}, er: {}'.format(n, lle.reconstruction_error_.round(7)))\n",
    "plt.suptitle('LLE (modified)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Isomap\n",
    "neighbours = [10,20,25,30,40,50,100,200]\n",
    "fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(25, 12))\n",
    "\n",
    "for n, ax in zip(neighbours, axes.flat):\n",
    "    isomap = manifold.Isomap(n_neighbors = n, n_components=2)\n",
    "    X_iso = isomap.fit_transform(X)\n",
    "    ax.scatter(X_iso[:, 0], X_iso[:,1], c=color)\n",
    "    ax.set_title('n= {} (error = {})'.format(n, isomap.reconstruction_error()))\n",
    "plt.suptitle('Isomap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[]\n",
    "for n in range(5,40):\n",
    "    isomap = manifold.Isomap(n_neighbors = n, n_components=2)\n",
    "    X_iso = isomap.fit_transform(X)\n",
    "    errors.append(isomap.reconstruction_error())\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print ('best number of neighbours = ', errors.index(min(errors))+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = manifold.Isomap(n_neighbors = 28, n_components=2)\n",
    "X_iso = isomap.fit_transform(X)\n",
    "plt.scatter(X_iso[:, 0], X_iso[:,1], c=color)\n",
    "plt.title('Isomap reconstruction')\n",
    "plt.show()\n",
    "print ('Reconstruction error:', isomap.reconstruction_error())\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one works better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "We see that our artificial data forms manifold in 3D space, which is twisted 2d - strip. Therefore we can reduce dimensionality of our data to 2D. In this case Isomap(because IsoMap makes several further assumptions – that the  data is actually located on a convex region in low-dimensional space and that it was embedded  in full space by some isometric transformation) and LLE methods seem the appropriate to reconstruct the data. however let's check all given  nonlinear methods and compare their results:\n",
    "\n",
    "<i> KernelPCA: </i> it fails, because given kernels don't represent the structure of manifold\n",
    "\n",
    "<i> Laplacian Eigenmaps: </i> it also failed\n",
    "\n",
    "<i> LLE: </i> LLE showed almost satisfactory results, because attempt to estimate the true geodesics: it simply assumes that a weighted  best-fit of each point’ s neighbors. However, we see that this assumption fails on the borders(yellows and purples) of our s-curve, because it consider neighbours that are only on the one side. Note: if we rescale data by extending one of features, we would achieve better results.\n",
    "\n",
    "<i> Isomap: </i> Shows the best result, because better estimate the true geodesics (manifold satisfies assumptions) and therefore better reproduct the local behaviour of manifold. \n",
    "\n",
    "LLE and Isomap significanly depends on the number of neighbours. \n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (20 pt.): Cocktail party problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to solve cocktail party problem. There is are two sound files, each of them contains a recording of two mixed sources. The trick is that in one recording one of the sources is closer to the microphone, in oter another source is closer. That property will help us to some extent to separate sound sources and generate two new files, for which the respective source will be more clear and loud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"party.jpg\" alt=\"Stamp Algo\" style=\"height: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have two sources and two microphones.\n",
    "\n",
    "Now try to use Independent Component Analysis in order to obtain independent components, each of which will hopefully correspond to its own source. The loaded data arrays have the following structure: a number corresponding to sampling rate and an array of amplitudes of a recording itself.\n",
    "\n",
    "Remember that you will have to **scale down your data** by a factor of $2^{15}$ so that that your observations are between -1 and 1. Also, do not forget to scale your data again *after transformation*. *You need to do this since the sound data is in 16-bit PCM WAV container.*\n",
    "\n",
    "Plot the time series for initial recordings and new transformed ones, and plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rate_1, data_1 = wavfile.read('rsm2_mA.wav')\n",
    "rate_2, data_2 = wavfile.read('rsm2_mB.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "from IPython.display import Audio\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "Audio(data_1, rate=rate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(data_2, rate=rate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scalling\n",
    "data_1_scalled = data_1 / (2**15)\n",
    "data_2_scalled = data_2 / (2**15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([data_1_scalled, data_2_scalled]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=2, whiten=True,random_state=42)\n",
    "S = ica.fit_transform(X)\n",
    "\n",
    "S = S.T\n",
    "data_1_transformed = S[0] * (2**15)\n",
    "data_2_transformed = S[1] * (2**15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, sharex=True,)\n",
    "\n",
    "axarr[0].plot(data_1)\n",
    "axarr[0].set_ylabel(\"Amplitude\")\n",
    "axarr[0].set_xlabel(\"Time\")\n",
    "axarr[0].set_title(\"original data\")\n",
    "\n",
    "axarr[1].plot(data_1_transformed)\n",
    "axarr[1].set_ylabel(\"Amplitude\")\n",
    "axarr[1].set_xlabel(\"Time\")\n",
    "axarr[1].set_title(\"Transformed data\")\n",
    "plt.title('audio 1')\n",
    "plt.subplots_adjust(top=1, bottom=0, hspace=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, sharex=True,)\n",
    "\n",
    "axarr[0].plot(data_2)\n",
    "axarr[0].set_ylabel(\"Amplitude\")\n",
    "axarr[0].set_xlabel(\"Time\")\n",
    "axarr[0].set_title(\"original data\")\n",
    "\n",
    "axarr[1].plot(data_2_transformed)\n",
    "axarr[1].set_ylabel(\"Amplitude\")\n",
    "axarr[1].set_xlabel(\"Time\")\n",
    "axarr[1].set_title(\"Transformed data\")\n",
    "plt.title('audio 2')\n",
    "plt.subplots_adjust(top=1, bottom=0, hspace=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(data_1_transformed, rate=rate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data_2_transformed, rate=rate_2)\n",
    "\n",
    "#END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might want to check how your algorithm performed and generate audio. **WARNING** be careful with the output file, if you messed up with amplitudes, the audio may be **extremely** loud. You are recommended not to use headphones and doublecheck whether your data arrays are scaled (TA experienced that himself and was not happy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (27 pt.): Nice theoretical bound on dimesionality to reduce to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we approach a theoretical task, where you will prove one interesting statistical dimensionality reduction result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you have $n$ points in $D$ dimensional space:\n",
    "\n",
    "$$u_1, ..., u_n \\in \\mathbb R^D.$$\n",
    "\n",
    "There exist a *linear* data transformation $F(u): \\mathbb R^D \\rightarrow \\mathbb R^d, D >> d$ such that:\n",
    "\n",
    "$$(1-\\delta) \\|u_i - u_j\\|^2 \\leq \\|F(u_i) - F(u_j)\\|^2\\leq (1+\\delta) \\|u_i - u_j\\|^2$$\n",
    "\n",
    "with high probability.\n",
    "\n",
    "The transformation $F(u)$ is: $F(u) = \\tfrac{1}{\\sqrt d}Xu$, and $X \\in \\mathbb R^{d \\times D}$ is a random matrix, components of which are independent identically distributed $X_{ij} \\sim \\mathcal{N}(0, 1)$.\n",
    "\n",
    "This statement means that there exists a matrix that reduces the original dimensionality such that pairwise distances are not distorted much. This is a theoretical probabilistic result that *guarantees* you such a transformation. We will obtain a bound on $d$ for which this result holds true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will need a little bit of theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful inequalitites and facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chernoff inequality.** This unequality states a bound on distribution tail.\n",
    "\n",
    "$$\\mathbb P(X \\geq t) \\leq \\frac{\\mathbb E \\exp(\\lambda X)}{\\exp(\\lambda t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition.** Random variable $X$ is called subexponential with parameters $(\\sigma^2, b)$, if for all $\\lambda: |\\lambda| < \\frac{1}{b}$ the following is true:\n",
    "\n",
    "$$\\mathbb E \\exp(\\lambda (X - \\mathbb E X)) \\leq \\exp\\left(\\frac{\\lambda^2\\sigma^2}{2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact.** $\\chi^2$ distribution with $d$ degrees of freedom is a sum of $d$ independent standard gaussian random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact.** $\\chi^2$ distribution with $d$ degrees of freedom is subexponential with parameters $\\sigma^2 = 4d, b = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 (5 pt.):\n",
    "\n",
    "Using the above information, prove that for $Y \\sim \\chi^2$ with $d$ degrees of freedom the following inequality holds true:\n",
    "\n",
    "$$\\mathbb P (\\lvert Y - d \\rvert \\geq t) \\leq 2\\exp\\left(\\frac{-t^2}{8d}\\right)$$\n",
    "\n",
    "for $t \\leq \\frac{\\sigma^2}{b}$. \n",
    "\n",
    "**Hint**: you will need to optimise the power of exponential in order to get optimal $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "1) $\\mathbb E Y= d$\n",
    "\n",
    "2) $\\mathbb P \\left( Y -d \\geq t \\right ) = \\mathbb P \\left( Y \\geq t+d \\right) \\leq \\left[ \\text{Chernoff inequality} \\right] \\leq \\frac{\\mathbb E e^{\\lambda Y}}{e^{\\lambda (t+d)}} = \\frac{\\mathbb E e^{\\lambda (Y -  d)}}{e^{\\lambda t}} \\leq \\left[ \\text{Sub-exponential} \\right] \\leq e^{\\inf_{\\lambda \\in (-\\frac{1}{b}, \\frac{1}{b})} \\left( \\frac{\\lambda^2 \\sigma^2}{2} - \\lambda t\\right) }  = e^{\\inf_{\\lambda \\in (-\\frac{1}{b}, \\frac{1}{b})} f(\\lambda, t) }$ \n",
    "\n",
    "3) $f(\\lambda, t) = \\frac{\\lambda^2 \\sigma^2}{2} - \\lambda t  \\quad \\Rightarrow \\quad \\frac{\\partial f(\\lambda,t)}{\\partial \\lambda} = \\lambda \\sigma^2 - t = 0 \\quad \\Rightarrow \\quad \\lambda = \\frac{t}{\\sigma^2}$\n",
    "\n",
    "4) $- \\frac{1}{b} < \\lambda < \\frac{1}{b} \\Rightarrow - \\frac{1}{b} < \\frac{t}{\\sigma^2} < \\frac{1}{b} \\text{and f is monotonic and continuous}\n",
    "    \\Rightarrow - \\frac{\\sigma^2}{b} \\leq t \\leq \\frac{\\sigma^2}{b} : \\inf_{|\\lambda| < \\frac{1}{b}}f(\\lambda,t) = f(\\frac{t}{\\sigma^2},t) = -\\frac{t^2}{2\\sigma^2}$\n",
    "    \n",
    "5) $\\mathbb P \\left( \\left|Y -d \\right| \\geq t \\right ) = 2  \\mathbb P \\left( Y -d \\geq t \\right ) \\leq 2e^{\\frac{-t^2}{2\\sigma^2}} = 2e^{\\frac{-t^2}{8d}}$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 (5 pt.):\n",
    "\n",
    "Prove that $\\frac{\\|Xu\\|^2}{\\|u\\|^2}$ is $\\chi_2$ random variable with $d$ dimensions of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2.1 (1 pt.) \n",
    "\n",
    "Show that $\\frac{X^T_iu}{\\|u\\|}$ is gaussian random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "$$ \\frac{X^T_iu}{\\|u\\|} = \\frac{\\sum X_{ij}u_j}{||u||} = \\sum \\frac{u_j}{||u||} X_{ij}$$  \n",
    "\n",
    " linear combination of gaussian random variables = gaussian r.v.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2.2 (2 pt.)\n",
    "\n",
    "Show that expectation $\\mathbb E \\frac {X^T_iu} {\\|u\\|} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "$$\\mathbb E\\frac{X^T_iu}{\\|u\\|} = \\mathbb E \\frac{\\sum X_{ij}u_j}{||u||} = \\sum \\frac{u_j}{||u||} \\mathbb E (X_{ij}) = 0 $$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2.3 (2 pt.)\n",
    "\n",
    "Show that variance $\\mathbb V \\frac {X^T_iu} {\\|u\\|} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "$$\\mathbb V\\frac{X^T_iu}{\\|u\\|} = \\mathbb V \\frac{\\sum X_{ij}u_j}{||u||} = \\sum \\mathbb V \\left (\\frac{u_j}{||u||}X_{ij} \\right)=\\sum \\frac{u_j^2}{||u||^2} \\mathbb V (X_{ij}) =  \\sum \\frac{u_j^2}{||u||^2} = 1$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we easily show that $\\frac{\\|Xu\\|^2}{\\|u\\|^2}$ is a sum of $d$ standard normal random variables, hence it is $\\chi_2$ random variable with $d$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3 (7 pt.)\n",
    "\n",
    "Having all of the previous results, we now may apply them to get the bound.\n",
    "\n",
    "From inequality for tails from Task 4.1 we get that:\n",
    "\n",
    "$$\\mathbb P \\left(\\left\\lvert \\frac{\\|Xu\\|^2}{\\|u\\|^2} - d \\right\\rvert \\geq t\\right) \\leq 2\\exp\\left(\\frac{-t^2}{8d}\\right)$$\n",
    "\n",
    "This means that probability of such event that our distances will change a lot is bounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3.1 (5 pt.)\n",
    "Show that if probability above is small, then probability of:\n",
    "\n",
    "$$(1-\\delta) \\|u_i - u_j\\|^2 \\leq \\|\\frac{1}{\\sqrt d} X(u_i - u_j)\\|^2\\leq (1+\\delta) \\|u_i - u_j\\|^2$$ \n",
    "\n",
    "is big and basically almost equal to $1 - n(n-1)\\exp \\left(-\\frac{\\delta^2d}{8}\\right)$.\n",
    "\n",
    "**Hints:**\n",
    "* at some point you would like to take $\\delta = \\frac{t}{d}$. Note that it makes $\\delta$ be in range of 0 and 1\n",
    "* use the independence of our data in order to transfer from single vector $u$ to differences $u_i - u_j$ and count number of such pairs correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "1) $\\mathbb P \\left( \\left| \\frac{\\| Xu\\|^2}{\\|u \\|^2} - d \\right| \\geq t \\right) \\leq 2e^{\\frac{-t^2}{8d}} \n",
    "\\quad \\Rightarrow \\quad \\mathbb P \\left( \\left| \\frac{\\| Xu\\|^2}{d\\|u \\|^2} - 1 \\right| \\geq \\frac{t}{d} \\right) \\leq 2e^{\\frac{-t^2}{8d}}$\n",
    "\n",
    "\n",
    "\n",
    "2) Let $\\delta = \\frac{t}{d}$ , then $0 < \\delta < 1$, because $0 < t < d$\n",
    "\n",
    "$\\Rightarrow \\quad \\mathbb P \\left( \\left| \\frac{\\| Xu\\|^2}{d\\|u \\|^2} - 1 \\right| \\geq \\delta \\right) \\leq 2e^{\\frac{-d\\delta^2}{8}} \\Leftrightarrow \\quad \n",
    "\\mathbb P \\left( \\frac{\\| Fu\\|^2}{\\|u\\|^2} \\in \\left[1-\\delta, 1+\\delta \\right]\\right) \\geq 1 -2e^{\\frac{-d\\delta^2}{8}}$\n",
    "\n",
    "\n",
    "3) Hence, for $C_n^2 = \\frac{n(n-1)}{2} $ pairs $u_i, u_j$ s.t: $u = u_i - u_j$:\n",
    "\n",
    "$$\\mathbb P \\left( \\exists i \\neq j : \\frac{\\| F(u_i - u_j)\\|^2}{\\|u_i - u_j\\|^2} \\in \\left[1-\\delta, 1+\\delta \\right] \\right) \\geq 1 - 2C_n^2 e^{\\frac{-d\\delta^2}{8}} = 1 - n(n-1)e^{\\frac{-d\\delta^2}{8}} $$\n",
    " \n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3.2 (2 pt.)\n",
    "\n",
    "After the previous step, we got that if we want to have our inequalities to be true with high probability $1-\\varepsilon$, we want the following:\n",
    "\n",
    "$$1-\\varepsilon \\leq 1 - n(n-1)\\exp \\left(-\\frac{\\delta^2d}{8}\\right).$$\n",
    "\n",
    "Derive the inequality for $d$ from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "$$1-\\varepsilon \\leq 1 - n(n-1)\\exp \\left(-\\frac{\\delta^2d}{8}\\right)$$\n",
    "\n",
    "$$\\varepsilon \\geq n(n-1) \\exp \\left(-\\frac{\\delta^2d}{8}\\right)$$\n",
    "\n",
    "$$\\exp \\left(-\\frac{\\delta^2d}{8}\\right) \\leq \\frac{\\varepsilon}{n(n-1)}$$\n",
    "\n",
    "$$\\frac{\\delta^2 d}{8} \\geq -ln \\frac{\\varepsilon}{n(n-1)}$$\n",
    "\n",
    "$$\\frac{\\delta^2 d}{8} \\geq ln \\frac{n(n-1)}{\\varepsilon}$$\n",
    "\n",
    "$$d \\geq \\frac{8}{\\delta^2} ln \\left(\\frac{n(n-1)}{\\varepsilon} \\right)$$\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.4 (10 pt.)\n",
    "\n",
    "We have sucessfully got our lower bound on the dimensionality we can safely reduce to such that pairwise distances do not change much: \n",
    "\n",
    "$$d \\geq \\frac{8}{\\delta^2}\\log\\left(\\frac{n(n-1)}{\\varepsilon}\\right)$$\n",
    "\n",
    "Note the beauty of that inequality. It **doesn't depend** on original dimensionality $D$, parameters $n$ -- number of samples and $\\varepsilon$ -- probability are under the $\\log$ function.\n",
    "\n",
    "This bound is not very tight, using more advanced techniques you may improve it. That means, our estimate of $d$ may be too high, but we can guarantee our result for it. Also remember that this approach is probabilistic, and, basically, depends on how lucky you will be with you data. It is very useful in case of extremely large dimensionalities $D$, and allows to reduce dimensionality while preserving the structure of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the obtained result and confirm that it holds true.\n",
    "\n",
    "* Fetch *20news* dataset\n",
    "* Check the dimensionality of data\n",
    "* Generate a random matrix of the corresponding size\n",
    "* Fix $\\delta$ = 0.15 and $\\varepsilon$ = 0.01\n",
    "* Show that distances of transformed data are within the delta tube\n",
    "* Remember that our result is not applicable in case if distance is 0\n",
    "* Show that for smaller $d$ this result doesn't work. You will have to take much smaller $d$ in order to show that (ten times less, for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100,10)\n",
    "n = X.shape[0]\n",
    "D = X.shape[1]\n",
    "delta = 0.15\n",
    "eps = 0.01\n",
    "d = 100\n",
    "M = np.random.normal(loc=0,scale=1, size=(d,D))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i!=j:\n",
    "            r = X[i,:] - X[j,:]\n",
    "            if r.sum()!=0.:\n",
    "                r = r.T\n",
    "                #print (r.shape)\n",
    "                objective = np.linalg.norm(1/np.sqrt(d) * M @ r)/np.linalg.norm(r)\n",
    "                output_1 = np.count_nonzero(objective > 1 + delta)\n",
    "                output_2 = np.count_nonzero(objective < 1 - delta)\n",
    "                \n",
    "print (output_1==0 ^ output_2==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = fetch_20newsgroups_vectorized().data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "D = X.shape[1]\n",
    "delta = 0.15\n",
    "eps = 0.01\n",
    "treshhold = 8/(delta * delta) * np.log(n*(n-1)/eps) \n",
    "print (treshhold)\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### BEGIN Solution\n",
    "np.random.seed(42)\n",
    "# >>> your solution here <<<\n",
    "d = 6100\n",
    "d_s = 600\n",
    "M = np.random.normal(loc=0,scale=1, size=(d,D))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i!=j:\n",
    "            r = X[i,:] - X[j,:]\n",
    "            if r.sum()!=0.:\n",
    "                r = r.T\n",
    "                objective = np.linalg.norm(1/np.sqrt(d) * M @ r)/np.linalg.norm(r)\n",
    "                output_1 = np.count_nonzero(objective > 1 + delta)\n",
    "                output_2 = np.count_nonzero(objective < 1 - delta)\n",
    "                \n",
    "print (output_1==0 ^ output_2==0)\n",
    "\n",
    "### END Solution'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (38 + 5 pt.): Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you are asked to play around with different aspects of neural networks using PyTorch and try to understand their influence on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell only once\n",
    "#!conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (3 pt.): Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will understand how and why use different [activation functions](http://pytorch.org/docs/master/nn.html#non-linear-activations). For each of three following ponts, plot functions on one figure.\n",
    "\n",
    "* ReLU, ELU, Softplus;\n",
    "* Sigmoid, Softsign, Tanh;\n",
    "* Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "x = Variable(torch.linspace(-6,6))\n",
    "relu = nn.ReLU()\n",
    "elu = nn.ELU()\n",
    "softplus = nn.Softplus()\n",
    "sigmoid = nn.Sigmoid()\n",
    "softsign = nn.Softsign()\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "y_relu = relu(x)\n",
    "y_elu = elu(x)\n",
    "y_softplus = softplus(x)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_softsign = softsign(x)\n",
    "y_tanh = tanh(x)\n",
    "y_softmax = softmax(x)\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(20, 5))\n",
    "\n",
    "axes[0].plot(x.data.numpy(), y_relu.data.numpy(), label = \"RELU\")\n",
    "axes[0].plot(x.data.numpy(), y_elu.data.numpy(), label = \"ELU\")\n",
    "axes[0].plot(x.data.numpy(), y_softplus.data.numpy(), label = \"Softplus\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(x.data.numpy(), y_sigmoid.data.numpy(), label = \"Sigmoid\")\n",
    "axes[1].plot(x.data.numpy(), y_softsign.data.numpy(), label = \"Softsign\")\n",
    "axes[1].plot(x.data.numpy(), y_tanh.data.numpy(), label = \"Tanh\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(x.data.numpy(), y_softmax.data.numpy(), label = \"Softmax\")\n",
    "axes[2].legend()\n",
    "plt.show()\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (35 + 5 pt.): Your own Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This big task is intended to familiarize you with how different NN architectures perform, what may be changed, and what effect that may cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (15 pt.): Define your architecture and check how it works for classification of moons dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to code a working prototype of your network.\n",
    "\n",
    "* Fill in the class MLPNet. Set SGD optimizer\n",
    "* Import and generate moons dataset with 2000 samples, noise = 0.3 and random state = 0. Standardize you data.\n",
    "* Split data in train and test sets 80/20, train your model and see how it performs on test.\n",
    "* Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(mlp, X, y, epochs=1, lr=.2, batch_size=101):\n",
    "    # fill in fit method. do not change batch iterator\n",
    "    ### your setup here\n",
    "    solver = optim.SGD(mlp.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_batches = (len(X) + batch_size - 1) // batch_size\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        for i in range(n_batches):\n",
    "            slice_ = np.s_[i::n_batches]\n",
    "            X_batch = Variable(torch.from_numpy(X[slice_]).float())\n",
    "            y_batch = Variable(torch.from_numpy(y[slice_]).long())\n",
    "            \n",
    "            ### BEGIN: your optim step here. do not forget to reset gradients\n",
    "            prediction = mlp(X_batch)\n",
    "            loss_f = loss(prediction, y_batch)\n",
    "            solver.zero_grad()\n",
    "            loss_f.backward()\n",
    "            solver.step()            \n",
    "            ### END\n",
    "\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "    #### Your net here\n",
    "    nn.Linear(2,20), nn.Softplus(),\n",
    "    nn.Linear(20,2), nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=2000, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# in order to obtain your predictions after fit, use\n",
    "mlp = fit(mlp,X = X_train,y=y_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score\n",
    "\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "out = mlp(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "# fancy performance report\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test,predicted))\n",
    "print ('Confusion matrix:')\n",
    "print (df_cm)\n",
    "\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(a, b, factor=0.2):\n",
    "    \"\"\"Make a wider interval defined by the endpoints.\"\"\"\n",
    "    return a - abs(a) * factor, b + abs(b) * factor\n",
    "\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())\n",
    "out = mlp(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "#print (predictions)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter', alpha=0.4)\n",
    "plt.title('Decision boundary and training sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (10 + 5 pt.): Changing the parameters.\n",
    "\n",
    "* First, see how your baseline network depends on data. Increase the number of samples, batch size and number of epochs\n",
    "* For the original data and your initial architecture play with learning rate, implement decreasing learning rate over the epochs.\n",
    "* Choose Adam optimizer. See how it performs for different learning rates\n",
    "* Write down your observations (up to 5 extra grading points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#More daaaata\n",
    "X, y = make_moons(n_samples=4000, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#fitting\n",
    "net =nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net = fit(net,X = X_train,y=y_train, epochs=3000)\n",
    "\n",
    "#test\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test,predicted))\n",
    "print(classification_report(y_test,predicted))\n",
    "\n",
    "#boudary\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())\n",
    "out = net(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter', s = 5, alpha = 0.5)\n",
    "plt.title('Decision boundary and training sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data - better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=2000, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "learning_rate = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "for lr,ax in zip(learning_rate, axes.flat):\n",
    "    net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "    net = fit(net,X = X_train,y=y_train, epochs=1000, lr=lr)\n",
    "    out = net(X_v)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    score = f1_score(y_test,predicted)\n",
    "    out = net(X_g)\n",
    "    predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('lr = {}, f1 = {}'.format(lr,score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of epochs\n",
    "n_epochs = [100, 200, 1000, 2000,3000,5000]\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "for ep,ax in zip(n_epochs, axes.flat):\n",
    "    net= nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "    net = fit(net,X = X_train,y=y_train, epochs=ep)\n",
    "    out = net(X_v)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    score = f1_score(y_test,predicted)\n",
    "    out = net(X_g)\n",
    "    predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('epochs = {}, f1 = {}'.format(ep,score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size\n",
    "batchsize = [20,50,100, 200]\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n",
    "for b,ax in zip(batchsize, axes.flat):\n",
    "    net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "    net = fit(net,X = X_train,y=y_train, epochs=1000, batch_size=b)\n",
    "    out = net(X_v)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    score = f1_score(y_test,predicted)\n",
    "    out = net(X_g)\n",
    "    predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('batchsize = {}, f1 = {}'.format(b,score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive learning rate\n",
    "def exp_lr_scheduler(optimizer, epoch, lr_decay=0.1, lr_decay_epoch=250):\n",
    "    if epoch % lr_decay_epoch:\n",
    "        return optimizer\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    return optimizer\n",
    "\n",
    "def fit_adaptive_lr(mlp, X, y, epochs=1, lr=.2, batch_size=101):\n",
    "    # fill in fit method. do not change batch iterator\n",
    "    ### your setup here\n",
    "    solver = optim.SGD(mlp.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_batches = (len(X) + batch_size - 1) // batch_size\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        for i in range(n_batches):\n",
    "            slice_ = np.s_[i::n_batches]\n",
    "            X_batch = Variable(torch.from_numpy(X[slice_]).float())\n",
    "            y_batch = Variable(torch.from_numpy(y[slice_]).long())\n",
    "            \n",
    "            ### BEGIN: your optim step here. do not forget to reset gradients\n",
    "            prediction = mlp(X_batch)\n",
    "            loss_f = loss(prediction, y_batch)\n",
    "            solver.zero_grad()\n",
    "            loss_f.backward()\n",
    "            solver.step()\n",
    "            \n",
    "        exp_lr_scheduler(solver,epoch)\n",
    "            ### END\n",
    "\n",
    "    return mlp\n",
    "net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net= fit_adaptive_lr(net, X_train, y_train, epochs=3000)\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "# fancy performance report\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test,predicted))\n",
    "print ('Confusion matrix:')\n",
    "print (df_cm)\n",
    "\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_adam(mlp, X, y, epochs=1, lr=.2, batch_size=101):\n",
    "#     your new fit function here\n",
    "    solver = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_batches = (len(X) + batch_size - 1) // batch_size\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        for i in range(n_batches):\n",
    "            slice_ = np.s_[i::n_batches]\n",
    "            X_batch = Variable(torch.from_numpy(X[slice_]).float())\n",
    "            y_batch = Variable(torch.from_numpy(y[slice_]).long())\n",
    "            \n",
    "            ### BEGIN: your optim step here. do not forget to reset gradients\n",
    "            prediction = mlp(X_batch)\n",
    "            loss_f = loss(prediction, y_batch)\n",
    "            solver.zero_grad()\n",
    "            loss_f.backward()\n",
    "            solver.step()            \n",
    "            ### END\n",
    "\n",
    "    return mlp\n",
    "\n",
    "learning_rate = [0.01, 0.05 , 0.1, 0.2, 0.3, 0.5]\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "for lr,ax in zip(learning_rate, axes.flat):\n",
    "    net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "    net = fit_with_adam(net,X = X_train,y=y_train, epochs=1000, lr=lr)\n",
    "    out = net(X_v)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    score = f1_score(y_test,predicted)\n",
    "    out = net(X_g)\n",
    "    predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('lr = {}, f1 = {}'.format(lr,score))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "- more data - better results\n",
    "- more epochs - better result\n",
    "- we should find the trade-off for lr, batch size and number of epochs\n",
    "- MLP with Adam optimizer showed better results\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 (10 pt.): Compare with kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will see how your network performs compared to simple k Nearest Neighbors algorithm.\n",
    "\n",
    "* Use moons dataset with 100, 500, 1000, 5000 samples\n",
    "* For each datset train kNN algorithm with different numbers of neighbors between 2 and 10. Plot the data with decision boundary.\n",
    "* For each dataset train your best Neural Network. Plot the data with decision boundary.\n",
    "* Which one of these algorithms works better for smaller datasets? For larger ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=100, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = range(2,11)\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(15,10))\n",
    "for n,ax in zip(neighbours, axes.flat):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    prediction = knn.predict(X_test)\n",
    "    score = f1_score(y_test,prediction)\n",
    "    predictions = knn.predict_proba(X_grid)[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_standartized[:, 0], X_standartized[:, 1], c= y, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('n = {}, f1 = {}'.format(n,score))\n",
    "plt.show()\n",
    "net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net = fit(net,X = X_train,y=y_train, epochs=2000, lr=0.2, batch_size=10)\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "score = f1_score(y_test,predicted)\n",
    "out = net(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "#print (predictions)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_standartized[:, 0], X_standartized[:, 1], c=y, cmap = 'winter', alpha=0.4)\n",
    "plt.title('Decision boundary and training sample, f1 = {}'.format(score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(15,10))\n",
    "for n,ax in zip(neighbours, axes.flat):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    prediction = knn.predict(X_test)\n",
    "    score = f1_score(y_test,prediction)\n",
    "    predictions = knn.predict_proba(X_grid)[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_standartized[:, 0], X_standartized[:, 1], c= y, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('n = {}, f1 = {}'.format(n,score))\n",
    "plt.show()\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net = fit(net,X = X_train,y=y_train, epochs=3000, lr=0.2, batch_size=25)\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "score = f1_score(y_test,predicted)\n",
    "out = net(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "#print (predictions)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_standartized[:, 0], X_standartized[:, 1], c=y, cmap = 'winter', alpha=0.4)\n",
    "plt.title('Decision boundary and training sample, f1 = {}'.format(score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(15,10))\n",
    "for n,ax in zip(neighbours, axes.flat):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    prediction = knn.predict(X_test)\n",
    "    score = f1_score(y_test,prediction)\n",
    "    predictions = knn.predict_proba(X_grid)[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_standartized[:, 0], X_standartized[:, 1], c= y, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('n = {}, f1 = {}'.format(n,score))\n",
    "plt.show()\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,20), nn.Sigmoid(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net = fit(net,X = X_train,y=y_train, epochs=2000, lr=0.2, batch_size=10)\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "score = f1_score(y_test,predicted)\n",
    "out = net(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "#print (predictions)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_standartized[:, 0], X_standartized[:, 1], c=y, cmap = 'winter', alpha=0.4)\n",
    "plt.title('Decision boundary and training sample, f1 = {}'.format(score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=5000, shuffle=True, noise=0.3, random_state=0)\n",
    "X_standartized = (X-X.mean(axis = 0))/X.std(axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standartized, y, test_size=0.2, random_state=0)\n",
    "X_v = Variable(torch.Tensor(X_test).float())\n",
    "X_l, X_h = X.min(axis=0)-1, X.max(axis=0)+1\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=101),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=101))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "X_g = Variable(torch.Tensor(X_grid).float())\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(25,20))\n",
    "for n,ax in zip(neighbours, axes.flat):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    prediction = knn.predict(X_test)\n",
    "    score = f1_score(y_test,prediction)\n",
    "    predictions = knn.predict_proba(X_grid)[:,1].reshape(xx0.shape)\n",
    "    ax.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "    ax.axis('off')\n",
    "    ax.scatter(X_standartized[:, 0], X_standartized[:, 1], c= y, cmap = 'winter',s=10, alpha= 0.5 )\n",
    "    ax.set_title('n = {}, f1 = {}'.format(n,score))\n",
    "plt.show()\n",
    "net = nn.Sequential(nn.Linear(2,20), nn.Softplus(),nn.Linear(20,2), nn.Sigmoid())\n",
    "net = fit(net,X = X_train,y=y_train, epochs=3000, lr=0.2, batch_size=150)\n",
    "out = net(X_v)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "score = f1_score(y_test,predicted)\n",
    "out = net(X_g)\n",
    "predictions = out.data.numpy()[:,1].reshape(xx0.shape)\n",
    "#print (predictions)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.contourf(xx0, xx1, predictions, cmap = 'Wistia')\n",
    "plt.axis('off')\n",
    "plt.scatter(X_standartized[:, 0], X_standartized[:, 1], c=y, cmap = 'winter', alpha=0.4)\n",
    "plt.title('Decision boundary and training sample, f1 = {}'.format(score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
